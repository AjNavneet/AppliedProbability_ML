{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Probability for Multiple Random Variables\n",
        "\n",
        "Probability helps us measure the uncertainty of random variable outcomes. It's quite straightforward when dealing with a single variable, but in machine learning, we often work with several variables interacting in complex, unknown ways. To handle this, we use specific techniques to calculate the probabilities of multiple random variables, which include:\n",
        "\n",
        "- **Joint Probability**: This measures the likelihood of two or more events happening at the same time.\n",
        "\n",
        "- **Marginal Probability**: This quantifies the probability of an event without considering the outcomes of other variables.\n",
        "\n",
        "- **Conditional Probability**: It calculates the probability of an event when one or more other events have already occurred.\n",
        "\n",
        "In this tutorial, we'll provide a beginner-friendly introduction to these concepts, allowing you to gain a solid understanding of how they form the foundation for fitting predictive models to data.\n",
        "\n",
        "By the end of this tutorial, you'll have a clear grasp of:\n",
        "\n",
        "- Joint probability, which deals with the simultaneous occurrence of events.\n",
        "\n",
        "- Marginal probability, which focuses on single events independently of other variables.\n",
        "\n",
        "- Conditional probability, which evaluates the likelihood of an event given the presence of other events.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OxC-ppSNChnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability for a Single Random Variable\n",
        "\n",
        "Probability is all about measuring the likelihood of an event. It tells us how probable a specific outcome is for a random variable, like flipping a coin, rolling a dice, or drawing a playing card from a deck.\n",
        "\n",
        "- In simple terms, probability is a way of quantifying the chances of something happening.\n",
        "  - As described in \"Probability: For the Enthusiastic Beginner\" (2016), it's about assessing the likelihood of an event taking place.\n",
        "\n",
        "- For a random variable 'x', we use a function P(x) to assign probabilities to all its possible values. This is expressed as the **Probability Density of x**, denoted as P(x).\n",
        "\n",
        "- When we want to find the probability of a specific event 'A' occurring for 'x', we use P(x = A) or simply P(A), where 'A' represents the event itself.\n",
        "\n",
        "- The calculation of probability is quite straightforward. We divide the number of desired outcomes by the total possible outcomes.\n",
        "  - This is especially intuitive for discrete random variables, like rolling a die. For instance, the probability of rolling a 5 on a six-sided die is calculated as the number of 5s (1) divided by the total possible outcomes (6), resulting in approximately 0.1666 or 16.666%.\n",
        "\n",
        "- It's crucial to remember that the sum of probabilities for all possible outcomes should always equal one, ensuring valid probabilities:\n",
        "\n",
        "  - **Sum of the Probabilities for All Outcomes = 1.0**\n",
        "\n",
        "- An impossible outcome carries a probability of zero. For instance, you can't roll a 7 with a standard six-sided die:\n",
        "\n",
        "  - **Probability of Impossible Outcome = 0.0**\n",
        "\n",
        "- On the other hand, a certain outcome has a probability of one. For example, when rolling a six-sided die, it's certain that a value between 1 and 6 will occur:\n",
        "\n",
        "  - **Probability of Certain Outcome = 1.0**\n",
        "\n",
        "- You can also calculate the probability of an event not happening, which is called the complement.\n",
        "  - This can be determined by subtracting the probability of the event from one, represented as 1 - P(A). For example, if you want to find the probability of not rolling a 5, it would be 1 - P(5), resulting in approximately 0.8333 or 83.333%.\n",
        "\n",
        "  - **P(not A) = 1 - P(A)**\n",
        "\n",
        "Now that we've covered the basics of probability for a single random variable, let's delve into probability for multiple random variables.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OJTTkwhHDI5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability with Multiple Random Variables\n",
        "\n",
        "- In the field of machine learning, we often work with many variables.\n",
        "  - Imagine a data table like one in Excel, where each row represents a different event or observation, and each column represents a distinct variable.\n",
        "  - These variables can be categorized as either discrete (taking on specific values) or continuous (having numerical values).\n",
        "\n",
        "- Dealing with multiple variables can be complex because they can interact in various ways, affecting their probabilities.\n",
        "  - To simplify this, let's focus on just two variables, X and Y, although the same principles apply to more variables. We'll also narrow our scope to the probability of two specific events: one for each variable (X = A, Y = B). However, we could equally discuss groups of events for each variable.\n",
        "\n",
        "- With this context, we introduce the probability of multiple random variables as the probability of event A and event B happening, represented as X = A and Y = B.\n",
        "  - We assume these two variables are related or dependent in some way. Consequently, there are three key types of probability to consider:\n",
        "\n",
        "    - **Joint Probability**: This measures the probability of both events A and B occurring together.\n",
        "\n",
        "    - **Marginal Probability**: It calculates the probability of event A happening, considering variable Y.\n",
        "\n",
        "    - **Conditional Probability**: This computes the probability of event A happening, given that event B has occurred.\n",
        "\n",
        "- These types of probability underpin many predictive modeling techniques in areas like classification and regression. For instance:\n",
        "\n",
        "  - The probability of a data row is essentially the combined probability across each input variable.\n",
        "\n",
        "  - The probability of a specific value for one input variable depends on the probabilities of the other input variables.\n",
        "\n",
        "  - The predictive model itself is an estimation of the probability of an output given an input example.\n",
        "\n",
        "Understanding joint, marginal, and conditional probability is fundamental in machine learning.\n",
        "\n",
        "Let's explore each of them in more detail.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8ojXHGVwD2KD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joint Probability: Simultaneous Events\n",
        "\n",
        "Sometimes, we want to understand the likelihood of two events occurring at the same time, such as the outcomes of two different random variables.\n",
        "  - This is referred to as the joint probability. When we consider the joint probability of two or more random variables, it's known as the joint probability distribution.\n",
        "  - To formally express the joint probability of event A and event B, we use the notation:\n",
        "\n",
        "    - **P(A and B)**\n",
        "\n",
        "  - Alternatively, the \"and\" or conjunction is denoted with the upside-down capital U (∩) or sometimes a comma (,). So, you might also see it as:\n",
        "\n",
        "    - **P(A ∩ B) = P(A, B)**\n",
        "\n",
        "- The joint probability for events A and B is calculated as the probability of event A given that event B has occurred, multiplied by the probability of event B. In a formal sense, it can be expressed as:\n",
        "\n",
        "    - **P(A ∩ B) = P(A given B) × P(B)**\n",
        "\n",
        "  - This calculation is sometimes known as the fundamental rule of probability or the product rule of probability.\n",
        "  - Here, P(A given B) represents the probability of event A given that event B has occurred, which is called the conditional probability (explained below).\n",
        "\n",
        "- It's important to note that the joint probability is symmetrical, meaning that **P(A ∩ B)** is the same as **P(B ∩ A)**.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "M85_M7K1EqDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Marginal Probability: Probability of an Event for One Variable\n",
        "\n",
        "Sometimes, we want to know the probability of an event for one random variable, regardless of what's happening with another random variable.\n",
        "- For instance, we might be interested in the probability of X = A for all the possible outcomes of Y. This concept is known as marginal probability, or the marginal distribution.\n",
        "\n",
        "- When we talk about the marginal probability of one random variable in the presence of additional random variables, it's referred to as the marginal probability distribution.\n",
        "\n",
        "- The term \"marginal\" is used because if you were to arrange all the outcomes and their probabilities for the two variables in a table (with X as columns and Y as rows),\n",
        "  - the marginal probability of one variable (X) would be the sum of the probabilities for the other variable (Y rows) on the edge of the table.\n",
        "\n",
        "  - There's no special notation for marginal probability; it's simply the sum or union over all the probabilities of all events for the second variable when you have a fixed event for the first variable.\n",
        "\n",
        "- Mathematically, this can be expressed as:\n",
        "\n",
        "    - **P(X = A) = Σ P(X = A, Y = y) for all y in Y**\n",
        "\n",
        "  - This is another essential rule in probability known as the sum rule.\n",
        "\n",
        "- It's important to note that marginal probability differs from conditional probability (discussed next) because it considers the union of all events for the second variable, rather than the probability of a single event.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "y1n2P8WkFIJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Probability: Probability Given Another Event\n",
        "\n",
        "Sometimes, we're interested in the probability of one event occurring when we already know that another event has taken place.\n",
        "  - This is known as conditional probability.\n",
        "  - When we're dealing with the conditional probability of one or more random variables, we refer to it as the conditional probability distribution.\n",
        "\n",
        "- The idea here is to find the probability of an event A happening under the condition that event B has occurred. We formally express this as:\n",
        "\n",
        "  - **P(A given B)**\n",
        "\n",
        "- The term \"given\" is represented using the pipe (|) operator. In other words, you might see it written as:\n",
        "\n",
        "  - **P(A|B)**\n",
        "\n",
        "- Mathematically, the conditional probability for events A given event B is calculated as:\n",
        "\n",
        "  - **P(A|B) = P(A ∩ B) / P(B)**\n",
        "\n",
        "  - Here, it's essential to note that this calculation assumes that the probability of event B is not zero, meaning it's not impossible.\n",
        "    - The concept of event A given event B doesn't imply that event B has already happened or is certain. Instead, it signifies the probability of event A occurring after or in the presence of event B during a specific trial.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3qQEAMVtFwAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability for Independence and Exclusivity\n",
        "\n",
        "In the context of multiple random variables, there are two important scenarios to consider.\n",
        "- First, it's possible that these variables don't interact, meaning they are independent of each other.\n",
        "- Alternatively, these variables may interact, but their events don't happen at the same time, which we call exclusivity.\n",
        "\n",
        "We'll delve into understanding the probabilities associated with multiple random variables under these circumstances in this section.\n",
        "\n",
        "  ---\n",
        "  \n"
      ],
      "metadata": {
        "id": "XPzYd9OLGSax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Independence: When Variables Don't Affect Each Other\n",
        "\n",
        "When one variable isn't influenced by another, we call this independence or statistical independence. This concept significantly impacts how we calculate the probabilities of these two variables.\n",
        "\n",
        "- For instance, let's consider the joint probability of independent events A and B. In this case, it's essentially the probability of A multiplied by the probability of B. Mathematically, we express this as:\n",
        "\n",
        "  - **Joint Probability: P(A ∩ B) = P(A) × P(B)**\n",
        "\n",
        "- As you might intuit, when dealing with an independent random variable, the marginal probability for an event is simply the probability of that event. In other words, it's the probability of a single random variable that you're familiar with:\n",
        "\n",
        "  - **Marginal Probability: P(A)**\n",
        "\n",
        "  - The marginal probability of an independent variable is essentially just the probability of that independent variable.\n",
        "\n",
        "- Similarly, when variables are independent, the conditional probability of A given B is simply the probability of A, as the probability of B has no effect. This can be be described as:\n",
        "\n",
        "  - **Conditional Probability: P(A|B) = P(A)**\n",
        "\n",
        "- The idea of statistical independence may be familiar from the world of sampling.\n",
        "  - It assumes that one sample isn't influenced by previous samples and doesn't affect future ones.\n",
        "  - In many machine learning algorithms, the assumption is made that samples from a domain are independent of each other and come from the same probability distribution, often referred to as \"independent and identically distributed\" or simply \"i.i.d.\"\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "f-xEKL65Gdrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exclusivity: When Events Don't Overlap\n",
        "\n",
        "When one event happening prevents the occurrence of other events, we say that these events are mutually exclusive.\n",
        "- In simple terms, they don't overlap; they're like separate worlds.\n",
        "-  The probabilities of these events are disjoint, which means they can't interact - they are strictly independent.\n",
        "  - In fact, if event A and event B are mutually exclusive, the joint probability of both events happening is zero:\n",
        "\n",
        "    - **P(A ∩ B) = 0.0**\n",
        "\n",
        "- Instead, we describe the probability of an outcome as event A or event B happening, which can be formally stated as:\n",
        "\n",
        "  - **P(A or B) = P(A) + P(B)**\n",
        "\n",
        "- The \"or\" is also called a union and is denoted by the capital U letter (∪), so you might see it as:\n",
        "\n",
        "  - **P(A or B) = P(A ∪ B)**\n",
        "\n",
        "- Now, if the events are not mutually exclusive, meaning that they can both happen, we might be interested in the outcome of either event.\n",
        "  - In this case, the probability of non-mutually exclusive events is calculated as the probability of event A and the probability of event B, minus the probability of both events happening simultaneously. Mathematically, it can be expressed as:\n",
        "\n",
        "    - **P(A ∪ B) = P(A) + P(B) - P(A ∩ B)**\n",
        "\n",
        "This concept helps us understand how to deal with situations where events can both occur, or they are exclusive and don't overlap.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wvtLZK0tHDhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "\n",
        "**Books:**\n",
        "\n",
        "1. [Probability: For the Enthusiastic Beginner, 2016](https://amzn.to/2jULJsu)\n",
        "2. [Pattern Recognition and Machine Learning, 2006](https://amzn.to/2JwHE7I)\n",
        "3. [Machine Learning: A Probabilistic Perspective, 2012](https://amzn.to/2xKSTCP)\n",
        "\n",
        "**Articles:**\n",
        "\n",
        "1. [Probability, Wikipedia](https://en.wikipedia.org/wiki/Probability)\n",
        "2. [Notation in probability and statistics, Wikipedia](https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics)\n",
        "3. [Independence (probability theory), Wikipedia](https://en.wikipedia.org/wiki/Independence_(probability_theory))\n",
        "4. [Independent and identically distributed random variables, Wikipedia](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
        "5. [Mutual exclusivity, Wikipedia](https://en.wikipedia.org/wiki/Mutual_exclusivity)\n",
        "6. [Marginal distribution, Wikipedia](https://en.wikipedia.org/wiki/Marginal_distribution)\n",
        "7. [Joint probability distribution, Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)\n",
        "8. [Conditional probability, Wikipedia](https://en.wikipedia.org/wiki/Conditional_probability)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iFcHb-IYHf-t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybLCcH_oDIjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}